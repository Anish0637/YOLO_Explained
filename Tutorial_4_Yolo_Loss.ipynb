{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Yolo loss function\n",
    "#### This is the forth tutorial of a series of step-by-step walkthroughs of Yolo algorithm\n",
    "\n",
    "In this tutorial, we are going to focus on **YOLO2**, which is simpler to understand and easier to explain comparing to its sucessors. If you are looking for YOLO3/4, I still encourage you to understand YOLO2 first as later modifications are **very similar** to YOLO2 in terms of detection logics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some boilerplates code \n",
    "import tensorflow as tf \n",
    "import os, glob \n",
    "import numpy as np \n",
    "from YoloBackbone.yolo2 import getOffset, getIOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last tutorial we created training labels for Yolo by converting a list of box coordinates into a tensor with shape (conv H, conv W, anchor num, 6). In the 1st tutorial we learned about the output shape of Yolo, which is (conv H, conv W, anchor num * 5+class num). For Pascal VOC dataset, the author used 5 anchor boxes and reshape all images to 416 x 416, meaning the output of Yolo is **(13, 13, 125)** and label is of shape **(13, 13, 5, 6)** \n",
    "\n",
    "In this tutorial, we are going to implement the yolo loss function described in figure 1\n",
    "![loss](Misc/YoloLoss.jpg)\n",
    "<center>figure 1: yolo loss</center>\n",
    "\n",
    "The above loss function can be decomposed into three components: \n",
    "1. the **localization loss**: measure the quality of predicted box location and size\n",
    "2. the **classification loss**: measure the quality of object classification \n",
    "3. the **confidence loss**: measure the sensitivity of detection (are all objects are detected? How many false positives? )\n",
    "\n",
    "We need three major inputs of our loss function for reasons that I'll later explain: \n",
    "1. **features**: tensor, of shape (None, convH, convW, num anchor * (num class + 5) ) = y predicted \n",
    "2. **labels**: tensor, of shape (None, convH, convW, num anchor * (num class + 5) ) = y true  \n",
    "3. **anchors**: tensor, of shape (num anchor, 2), predetermined anchor shapes\n",
    "4. **hyperparam**: dict, tunnable loss hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Localization Loss\n",
    "To measure the quality of the predicted boxes, it is important to measure the following two things:\n",
    "1. The distance between the **predicted box center** and the **ground truth center**\n",
    "2. The difference in box **sizes** and **shapes**\n",
    "\n",
    "Here's a heuristic:\n",
    "<pre>\n",
    "    loss = 0\n",
    "    for row in range [0,conv H]:\n",
    "        for col in range [0, conv W]:\n",
    "            for anchor k in range [0, 5]: # assume 5 anchors \n",
    "                if label[row, col,k] has a ground truth box:\n",
    "                    l1 = calculate the difference in center coordinates \n",
    "                    l2 = calculate the difference in box width and height\n",
    "                    loss += (l1 + l2)\n",
    "</pre>\n",
    "\n",
    "Of course, the iterative-based algorithm is going to be catastrophically slow. Some efforts are needed to vectorize the above heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from Utilities.io import LoadPascal\n",
    "import tensorflow.keras as tfk \n",
    "DATA_DIR = 'SampleData/Images'\n",
    "ANNOT_DIR = 'SampleData/Annotations'\n",
    "ANCHOR_PATH = 'SampleData/vocAnchors.txt'\n",
    "CLASSNAME_PATH = 'SampleData/vocClasses.txt'\n",
    "IMG_SHAPE = (416, 416, 3)\n",
    "loader = LoadPascal(imgDir=DATA_DIR, annotDir=ANNOT_DIR, \n",
    "                    anchorPath=ANCHOR_PATH, classNamePath=CLASSNAME_PATH)\n",
    "data = loader.loadData(imgShape=IMG_SHAPE, batchSize=2, imgOnly=False, shuffle=True)\n",
    "MODEL_PATH = 'I:/model/YOLO/tutorial/yolo_voc.h5'\n",
    "model = tfk.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample batch shape:(2, 416, 416, 3)\n",
      "sample label shape:(2, 13, 13, 5, 6)\n",
      "sample output shape:(2, 13, 13, 125)\n",
      "anchor shape:(5, 2)\n"
     ]
    }
   ],
   "source": [
    "# get a sample input \n",
    "for (imgs, labels, imgNames) in data:\n",
    "    break\n",
    "print('sample batch shape:{}'.format(imgs.shape))\n",
    "print('sample label shape:{}'.format(labels.shape))\n",
    "features = model.predict(imgs)\n",
    "print('sample output shape:{}'.format(features.shape))\n",
    "anchors = loader.anchors\n",
    "print('anchor shape:{}'.format(anchors.shape))\n",
    "hyperparam={'local': 5.0, 'obj': 5.0, 'nonObj': 0.5, 'iouThresh': 0.5}\n",
    "localCoef, nonObjCoef, objCoef = hyperparam['local'], hyperparam['nonObj'], hyperparam['obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. boxXY shape: (2, 13, 13, 5, 2), gtXH shape:(2, 13, 13, 5, 2)\n",
      "2. boxWH shape: (2, 13, 13, 5, 2), gtWH shape:(2, 13, 13, 5, 2)\n",
      "3. localLoss shape: (2, 13, 13, 5, 2)\n",
      "4. mask shape: (2, 13, 13, 5, 1)\n",
      "5. reduced local loss shape:(2,)\n"
     ]
    }
   ],
   "source": [
    "# local loss \n",
    "numAnchor = anchors.shape[0]\n",
    "numClass = fShape[-1] // numAnchor - 5\n",
    "fShape = tf.shape(features) # get feature shape\n",
    "features = tf.reshape(features, shape=[fShape[0], fShape[1], fShape[2], numAnchor, numClass + 5])\n",
    "\"\"\"\n",
    "converting tx, ty to bx, by(see figure 2)\n",
    "converting tw, th to bw, bh\n",
    "getOffset is implemented in tutorial 3\n",
    "\"\"\"\n",
    "offset = tf.cast(getOffset([fShape[1], fShape[2]]), features.dtype)\n",
    "boxXY = tf.nn.sigmoid(features[..., :2]) + offset\n",
    "boxWH = tf.math.exp(features[..., 2:4]) * anchors\n",
    "\"\"\"\n",
    "get ground truth coordinates and sizes. Remember gtXY and gtWH are created to match the scale of boxXY and boxWH \n",
    "in tutoral 3\n",
    "\"\"\"\n",
    "gtXY, gtWH = labels[..., 0:2] + offset, (labels[..., 2:4] * anchors)\n",
    "\"\"\"\n",
    "calculate the euclidean distance. local loss should be 13 by 13 in shape \n",
    "\"\"\"\n",
    "localLoss = tf.square(gtXY - boxXY) + tf.square(tf.sqrt(gtWH) - tf.sqrt(boxWH))\n",
    "print('1. boxXY shape: {}, gtXH shape:{}'.format(boxXY.shape, gtXY.shape))\n",
    "print('2. boxWH shape: {}, gtWH shape:{}'.format(boxWH.shape, gtWH.shape))\n",
    "print('3. localLoss shape: {}'.format(localLoss.shape))\n",
    "\"\"\"\n",
    "filter out grids that do not contain ground truth boxes by leveraging the the flag in label[i,j,k,0]\n",
    "\"\"\"\n",
    "mask = labels[..., 0:1]  # signal if an object appears in a given location\n",
    "print('4. mask shape: {}'.format(mask.shape))\n",
    "localLoss = tf.math.reduce_sum(localLoss, axis=[1, 2, 3, 4])\n",
    "print('5. reduced local loss shape:{}'.format(localLoss.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BoxPred](Misc/BoxPred.jpg)  \n",
    "<center>figure 2</center>\n",
    "\n",
    "### Step 2. classification loss\n",
    "For every grid and every anchor channel that contains a ground truth box, we calculate the probabilistic differences\n",
    "<pre>\n",
    "for row in range [0, conv H]:\n",
    "    for col in range [0, conv W]:\n",
    "        for anchor in range [0, 5]:\n",
    "            if anchor has a box:\n",
    "                measure differences\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. classProb shape: (2, 13, 13, 5, 20)\n",
      "2. grount truth prob shape: (2, 13, 13, 5, 20)\n",
      "3. classification loss shape: (2, 13, 13, 5, 20)\n",
      "4. classification loss reduced: (2,)\n"
     ]
    }
   ],
   "source": [
    "classProb = tf.nn.softmax(features[..., 5:])\n",
    "\"\"\"\n",
    "get the ground truth object class index and one hot encode it\n",
    "\"\"\"\n",
    "gtClasses = tf.cast(labels[..., 4], tf.int32)\n",
    "gtClasses = tf.one_hot(gtClasses, depth=numClass)\n",
    "print('1. classProb shape: {}'.format(classProb.shape))\n",
    "print('2. grount truth prob shape: {}'.format(gtClasses.shape))\n",
    "\"\"\"\n",
    "use the same mask trick to filter out locations without boxes\n",
    "\"\"\"\n",
    "\n",
    "classLoss = tf.square(classProb - gtClasses) * mask\n",
    "print('3. classification loss shape: {}'.format(classLoss.shape))\n",
    "classLoss = tf.math.reduce_sum(classLoss, axis=[1, 2, 3, 4])\n",
    "print('4. classification loss reduced: {}'.format(classLoss.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Confidence loss \n",
    "The confidence loss is slightly more complicated, as it not only measures the prediction is anchors channels that contain ground truth boxes. Moreover, we need to calculate the ground truth C by measuring the IOU of the boxes and anchors. \n",
    "<pre>\n",
    "for row in range [0, conv H]:\n",
    "    for col in range [0, conv W]:\n",
    "        for anchor in range [0, 5]:\n",
    "            if anchor has a box:\n",
    "                calculate Ci\n",
    "            if anchor has no box:\n",
    "                Ci = 0\n",
    "            measure the differences between Ci and Ci_pred\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. objScore shape:(2, 13, 13, 5, 1)\n",
      "2. IOU shape:(2, 13, 13, 5)\n",
      "3. nonObjLoss shape:(2, 13, 13, 5, 1)\n",
      "4. obj loss shape:(2, 13, 13, 5, 1)\n",
      "5. confidence loss reduced:(2,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "get predicted object score\n",
    "\"\"\"\n",
    "objScore = tf.nn.sigmoid(features[..., 4:5])\n",
    "print('1. objScore shape:{}'.format(objScore.shape))\n",
    "\"\"\"\n",
    "for a given grid in the label, there are at most one anchor channel that has a bounding box.\n",
    "However, that doesn't mean Ci=0 for other anchor channels in the same grid. After all, they overlap\n",
    "with the ground truth as well. \n",
    "\"\"\"\n",
    "iou = getIOU(tf.concat([boxXY, boxWH], axis=-1), tf.concat([gtXY, gtWH], axis=-1))\n",
    "print('2. IOU shape:{}'.format(iou.shape))\n",
    "\n",
    "nonObj = (1.0 - mask) * nonObjCoef\n",
    "nonObjLoss = nonObj * tf.square(0 - objScore)\n",
    "print('3. nonObjLoss shape:{}'.format(nonObjLoss.shape))\n",
    "objLoss = mask * tf.square(objScore - tf.expand_dims(iou, axis=-1)) * objCoef\n",
    "print('4. obj loss shape:{}'.format(objLoss.shape))\n",
    "objLoss = tf.math.reduce_sum(nonObjLoss + objLoss, axis=[1, 2, 3, 4])\n",
    "print('5. confidence loss reduced:{}'.format(objLoss.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yoloLoss(features, labels, anchors, hyperparam={'local': 5.0, 'obj': 5.0, 'nonObj': 0.5, 'iouThresh': 0.5}):\n",
    "    # get hyperparameters for the loss function\n",
    "    localCoef, nonObjCoef, objCoef = hyperparam['local'], hyperparam['nonObj'], hyperparam['obj']\n",
    "    fShape = tf.shape(features)\n",
    "    # numAnchor = labels.shape[-2] # label has shape (None, convH, convW, num anchor, 6)\n",
    "    numAnchor = anchors.shape[0]\n",
    "    numClass = fShape[-1] // numAnchor - 5\n",
    "    features = tf.reshape(features, shape=[fShape[0], fShape[1], fShape[2], numAnchor, numClass + 5])\n",
    "\n",
    "    # convert raw features into box coordinates w.r.t to each square in the feature map\n",
    "    # please refer to https://arxiv.org/pdf/1612.08242.pdf page 3\n",
    "    offset = tf.cast(getOffset([fShape[1], fShape[2]]), features.dtype)\n",
    "    boxXY = tf.nn.sigmoid(features[..., :2]) + offset\n",
    "    boxWH = tf.math.exp(features[..., 2:4]) * anchors\n",
    "    objScore = tf.nn.sigmoid(features[..., 4:5])\n",
    "    classProb = tf.nn.softmax(features[..., 5:])\n",
    "    # yolo loss is defined as\n",
    "    # W1 * localization loss + W2 * confidence loss + classification loss\n",
    "    # https://arxiv.org/abs/1506.02640 page 4\n",
    "\n",
    "    # calculate the classification loss\n",
    "    mask = labels[..., 0:1]  # signal if an object appears in a given location\n",
    "    labels = labels[..., 1:]\n",
    "    gtClasses = tf.cast(labels[..., 4], tf.int32)\n",
    "    gtClasses = tf.one_hot(gtClasses, depth=numClass)\n",
    "    classLoss = tf.square(classProb - gtClasses) * mask\n",
    "    classLoss = tf.math.reduce_sum(classLoss, axis=[1, 2, 3, 4])\n",
    "\n",
    "    # calculate the localization loss\n",
    "    # coordinates w.r.t to each square in the feature map\n",
    "    gtXY, gtWH = labels[..., 0:2] + offset, (labels[..., 2:4] * anchors)\n",
    "    localLoss = tf.square(gtXY - boxXY) + tf.square(tf.sqrt(gtWH) - tf.sqrt(boxWH))\n",
    "    localLoss *= mask\n",
    "    localLoss = localCoef * tf.math.reduce_sum(localLoss, axis=[1, 2, 3, 4])\n",
    "\n",
    "    # calculate the detection loss\n",
    "    iou = getIOU(tf.concat([boxXY, boxWH], axis=-1), tf.concat([gtXY, gtWH], axis=-1))\n",
    "    bestIOU = tf.math.reduce_max(iou, axis=-1, keepdims=True)\n",
    "    nonObj = bestIOU < hyperparam['iouThresh']\n",
    "    nonObj = tf.cast(tf.expand_dims(nonObj, axis=-1), mask.dtype) * (1.0 - mask) * nonObjCoef\n",
    "    nonObjLoss = nonObj * tf.square(0 - objScore)\n",
    "    objLoss = mask * tf.square(objScore - tf.expand_dims(iou, axis=-1)) * objCoef\n",
    "\n",
    "    objLoss = tf.math.reduce_sum(nonObjLoss + objLoss, axis=[1, 2, 3, 4])\n",
    "    # overall loss\n",
    "    return tf.math.reduce_mean(objLoss + localLoss + classLoss), [objLoss, localLoss, classLoss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.819646, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7078857, shape=(), dtype=float32)\n",
      "tf.Tensor(6.215113, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for (imgs, labels, names) in data:\n",
    "    predicted = model.predict(imgs)\n",
    "    loss, [objLm, localL, confidenceL] = yoloLoss(predicted, labels, anchors)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
